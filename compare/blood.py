import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve
from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import os
import sys
import json
import pickle
import warnings
import traceback
from datetime import datetime
import jieba  # æ–°å¢ï¼šå¯¼å…¥ jieba ç”¨äºä¸­æ–‡åˆ†è¯
import random
import pandas as pd  # æ·»åŠ pandaså¯¼å…¥

warnings.filterwarnings('ignore')

# ====================================================================================
# START: REQUIRED MODULES / PLACEHOLDERS (ç¡®ä¿è¿™äº›ç±»çš„å®šä¹‰å­˜åœ¨å¹¶æ­£ç¡®)
# ====================================================================================

# Placeholder for HybridHyperedgeGenerator (æ¥è‡ª models.modules.hyper_generator)
class HybridHyperedgeGenerator(nn.Module):
    def __init__(self, num_modalities, input_dims, hidden_dim, top_k, threshold):
        super().__init__()
        self.modality_projections = nn.ModuleList([
            nn.Sequential(nn.Linear(dim, hidden_dim), nn.ReLU()) for dim in input_dims
        ])
        self.att_scorer = nn.Linear(hidden_dim * num_modalities, 1)
        self.top_k = top_k
        self.threshold = threshold

    def forward(self, features_list):
        batch_size = features_list[0].shape[0]
        device = features_list[0].device

        projected_features = [proj(feat) for proj, feat in zip(self.modality_projections, features_list)]
        combined_features = torch.cat(projected_features, dim=1)
        attention_scores = torch.sigmoid(self.att_scorer(combined_features).squeeze())  # Sigmoid on attention scores

        normalized_combined_features = F.normalize(torch.cat(features_list, dim=1), dim=1)
        similarity = torch.mm(normalized_combined_features, normalized_combined_features.t())

        num_hyperedges = batch_size
        H_final = torch.zeros(batch_size, num_hyperedges, device=device)

        actual_top_k = min(self.top_k, batch_size)
        _, top_k_indices = torch.topk(similarity, actual_top_k, dim=1)

        for i in range(batch_size):
            H_final[i, i] = 1.0
            for neighbor_idx in top_k_indices[i]:
                if neighbor_idx < batch_size:
                    H_final[neighbor_idx, i] = 1.0

        for i in range(num_hyperedges):
            if H_final[:, i].sum() == 0:
                H_final[i, i] = 1.0

        edge_weights = attention_scores
        if edge_weights.shape[0] != num_hyperedges:
            # Fallback if dimensions don't match, or use a more robust weighting
            edge_weights = torch.ones(num_hyperedges, device=device) * (
                attention_scores.mean() if attention_scores.numel() > 0 else 1.0)

        edge_weights = torch.clamp(edge_weights, min=1e-8)

        return H_final, edge_weights


# Corrected WaveletChebConv (å·²ä¿®æ­£è¿‡ç»´åº¦é—®é¢˜)
class WaveletChebConv(nn.Module):
    def __init__(self, in_dim, out_dim, K, tau):
        super().__init__()
        self.linear = nn.Linear(in_dim, out_dim)
        self.K = K
        self.tau = tau

    def forward(self, x, L):
        if self.K == 0:
            return self.linear(x)

        Tx_0 = x
        Tx_1 = torch.mm(L, x)

        cheb_terms = [Tx_0, Tx_1]

        for k in range(2, self.K):
            Tx_k = 2 * torch.mm(L, cheb_terms[-1]) - cheb_terms[-2]
            cheb_terms.append(Tx_k)

        summed_cheb_output = torch.sum(torch.stack(cheb_terms, dim=0), dim=0)

        return self.linear(summed_cheb_output)


# Placeholder for SpectralCutRegularizer
class SpectralCutRegularizer(nn.Module):
    def __init__(self, use_rayleigh=True, reduction='mean'):
        super().__init__()
        self.use_rayleigh = use_rayleigh
        self.reduction = reduction

    def forward(self, representations, H, Dv, De):
        if self.use_rayleigh:
            if representations.numel() == 0:
                return torch.tensor(0.0, device=representations.device)
            reg_loss = torch.norm(representations, p=2)
            if self.reduction == 'mean':
                reg_loss = reg_loss.mean()
            elif self.reduction == 'sum':
                reg_loss = reg_loss.sum()
            return reg_loss * 0.01
        else:
            return torch.tensor(0.0, device=representations.device)


# ====================================================================================
# END: REQUIRED MODULES / PLACEHOLDERS
# ====================================================================================


# å‡çº§åçš„ BiLSTM æ–‡æœ¬ç¼–ç å™¨ç±» - æ¥å—è¯åµŒå…¥åºåˆ—
class BiLSTMAwareTextEncoder(nn.Module):
    """
    åŸºäº BiLSTM çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå¤„ç†è¯åµŒå…¥åºåˆ—ã€‚
    """

    def __init__(self, vocab_size, embedding_dim, hidden_dim, lstm_layers=1, dropout=0.2, bidirectional=True,
                 max_seq_len=64):
        super(BiLSTMAwareTextEncoder, self).__init__()

        self.embedding_dim = embedding_dim
        self.lstm_hidden_size = hidden_dim
        self.num_layers = lstm_layers
        self.bidirectional = bidirectional
        self.num_directions = 2 if bidirectional else 1
        self.max_seq_len = max_seq_len

        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=self.lstm_hidden_size,
            num_layers=self.num_layers,
            batch_first=True,
            bidirectional=self.bidirectional,
            dropout=dropout if self.num_layers > 1 else 0
        )

        self.output_projection = nn.Linear(self.num_directions * self.lstm_hidden_size, hidden_dim)

        self.layer_norm = nn.LayerNorm(hidden_dim)

    def forward(self, text_indices):
        embeddings = self.embedding(text_indices)

        output, (hn, cn) = self.lstm(embeddings)

        if self.bidirectional:
            final_hidden_state = torch.cat((hn[-2, :, :], hn[-1, :, :]), dim=1)
        else:
            final_hidden_state = hn[-1, :, :]

        final_features = self.output_projection(final_hidden_state)
        final_features = self.layer_norm(final_features)

        return final_features


class AblationAnomalyDetector(nn.Module):
    """æ”¯æŒæ¶ˆèå®éªŒçš„å¤šæ¨¡æ€è¶…å›¾å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, config, ablation_config):
        super(AblationAnomalyDetector, self).__init__()
        self.config = config
        self.ablation_config = ablation_config

        self.image_encoder = nn.ModuleDict({
            'adaptive_pool': nn.AdaptiveAvgPool2d((8, 8)),  # ç»Ÿä¸€è¾“å‡ºä¸º8x8ä»¥å‡å°‘ç»´åº¦
            'flatten': nn.Flatten(),
            'projection': nn.Sequential(
                nn.Linear(3 * 8 * 8, config['hidden_dim']),  # 3é€šé“ * 8 * 8 = 192
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(config['hidden_dim'], config['hidden_dim']),
                nn.BatchNorm1d(config['hidden_dim'])
            )
        })

        self.text_encoder_module = BiLSTMAwareTextEncoder(
            vocab_size=config['vocab_size'],
            embedding_dim=config['embedding_dim'],
            hidden_dim=config['hidden_dim'],
            lstm_layers=config.get('lstm_layers', 1),
            dropout=config.get('lstm_dropout', 0.2),
            bidirectional=True,
            max_seq_len=config['max_seq_len']
        )

        input_dims = [config['hidden_dim']] * 2
        if ablation_config['hypergraph_type'] == 'dynamic':
            self.hypergraph_generator = HybridHyperedgeGenerator(
                num_modalities=2,
                input_dims=input_dims,
                hidden_dim=config['hidden_dim'],
                top_k=config.get('top_k', 8),
                threshold=config.get('threshold', 0.6)
            )

        conv_input_dim = config['hidden_dim'] * 2

        self.feature_adapter = nn.Linear(conv_input_dim, conv_input_dim)

        if ablation_config['conv_type'] == 'wavelet_cheb':
            self.conv1 = WaveletChebConv(
                in_dim=conv_input_dim,
                out_dim=config['hidden_dim'],
                K=config.get('cheb_k', 3),
                tau=config.get('tau', 0.5)
            )
            self.conv2 = WaveletChebConv(
                in_dim=config['hidden_dim'],
                out_dim=config['repr_dim'],
                K=config.get('cheb_k', 3),
                tau=config.get('tau', 0.5)
            )

        self.representation_layer = nn.Sequential(
            nn.Linear(config['repr_dim'], config['repr_dim'] // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(config['repr_dim'] // 2, config['final_repr_dim']),
            nn.BatchNorm1d(config['final_repr_dim'])
        )

        self.anomaly_scorer = nn.Sequential(
            nn.Linear(config['final_repr_dim'], config['final_repr_dim'] // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(config['final_repr_dim'] // 2, 1),
            nn.Sigmoid()
        )

        self.reconstructor = nn.ModuleDict({
            'hidden_projection': nn.Linear(config['final_repr_dim'], config['hidden_dim']),
            'output_projection': None
        })

        if ablation_config['use_spectral_regularizer']:
            self.pruning_regularizer = SpectralCutRegularizer(
                use_rayleigh=True,
                reduction='mean'
            )

    def forward(self, images, text_sequences, labels=None, return_all=False):  # text_features æ”¹ä¸º text_sequences
        batch_size = images.shape[0]
        device = images.device

        # Image processing block - ensuring 3-channel 28x28
        if len(images.shape) == 2:
            target_pixels = 3 * 28 * 28
            if images.shape[1] >= target_pixels:
                images = images[:, :target_pixels].view(batch_size, 3, 28, 28)
            else:
                padding_size = target_pixels - images.shape[1]
                padded_images = torch.cat([images, torch.zeros(batch_size, padding_size, device=device)], dim=1)
                images = padded_images.view(batch_size, 3, 28, 28)

        if images.shape[1] != 3:
            if images.shape[1] == 1:
                images = images.repeat(1, 3, 1, 1)
            elif images.shape[1] > 3:
                images = images[:, :3, :, :]
            else:
                padding_channels = 3 - images.shape[1]
                padding = torch.zeros(batch_size, padding_channels, images.shape[2], images.shape[3], device=device)
                images = torch.cat([images, padding], dim=1)

        img_pooled = self.image_encoder['adaptive_pool'](images)
        img_flattened = self.image_encoder['flatten'](img_pooled)
        img_features = self.image_encoder['projection'](img_flattened)

        # Pass text_sequences (integer IDs) to BiLSTM
        text_feat = self.text_encoder_module(text_sequences)

        features_list = [img_features, text_feat]

        H, edge_weights = self.hypergraph_generator(features_list)

        L = self._compute_hypergraph_laplacian(H, edge_weights)

        node_features = torch.cat(features_list, dim=1)

        actual_feature_dim = node_features.shape[1]
        if not hasattr(self, '_adapted_feature_dim'):
            self._adapted_feature_dim = actual_feature_dim
            if self.reconstructor['output_projection'] is None:
                self.reconstructor['output_projection'] = nn.Sequential(
                    nn.ReLU(),
                    nn.Linear(self.config['hidden_dim'], actual_feature_dim)
                ).to(node_features.device)

        x1 = self.conv1(node_features, L)
        x1 = F.relu(x1)
        x1 = F.dropout(x1, p=0.2, training=self.training)

        x2 = self.conv2(x1, L)
        x2 = F.relu(x2)

        representations = self.representation_layer(x2)

        anomaly_scores = self.anomaly_scorer(representations).squeeze()

        losses = {}
        total_loss = torch.tensor(0.0, device=device)

        if labels is not None:
            detection_loss = F.binary_cross_entropy(anomaly_scores, labels.float())
            losses['detection_loss'] = detection_loss
            total_loss += detection_loss

            hidden_repr = self.reconstructor['hidden_projection'](representations)
            reconstructed = self.reconstructor['output_projection'](hidden_repr)
            original_features = node_features.detach()

            if reconstructed.shape != original_features.shape:
                print(f"é‡æ„ç»´åº¦è­¦å‘Š: reconstructed {reconstructed.shape} vs original {original_features.shape}")
                min_dim = min(reconstructed.shape[1], original_features.shape[1])
                reconstructed = reconstructed[:, :min_dim]
                original_features = original_features[:, :min_dim]

            recon_loss = F.mse_loss(reconstructed, original_features)
            losses['reconstruction_loss'] = recon_loss
            recon_weight = self.config.get('lambda_recon', 0.1)
            total_loss += recon_weight * recon_loss

            if self.ablation_config['use_spectral_regularizer']:
                Dv, De = self._compute_degree_matrices(H)
                spectral_loss = self.pruning_regularizer(representations, H, Dv, De)
                losses['spectral_loss'] = spectral_loss
                spectral_weight = self.config.get('lambda_spectral', 0.01)
                total_loss += spectral_weight * spectral_loss
            else:
                losses['spectral_loss'] = torch.tensor(0.0, device=device)

        losses['total_loss'] = total_loss

        if return_all:
            return {
                'anomaly_scores': anomaly_scores,
                'representations': representations,
                'hypergraph': (H, edge_weights),
                'losses': losses
            }
        else:
            return anomaly_scores, losses

    def _compute_hypergraph_laplacian(self, H, edge_weights):
        batch_size, num_edges = H.shape
        device = H.device

        if num_edges == 0:
            return torch.eye(batch_size, device=device)

        Dv, De = self._compute_degree_matrices(H)

        dv_diag = Dv.diag()
        de_diag = De.diag()

        if dv_diag.sum() == 0 or de_diag.sum() == 0:
            return torch.eye(batch_size, device=device)

        if edge_weights.numel() == 0:
            edge_weights = torch.ones(num_edges, device=device)
        W = torch.diag(edge_weights)

        dv_sqrt_inv = torch.diag(1.0 / (torch.sqrt(dv_diag + 1e-8)))
        de_inv = torch.diag(1.0 / (de_diag + 1e-8))

        try:
            Theta = torch.mm(torch.mm(torch.mm(torch.mm(dv_sqrt_inv, H), W), de_inv),
                             torch.mm(H.t(), dv_sqrt_inv))
        except RuntimeError as e:
            print(f"æ‹‰æ™®æ‹‰æ–¯è®¡ç®—é”™è¯¯: {e}")
            print(f"H shape: {H.shape}, edge_weights shape: {edge_weights.shape}")
            print(f"dv_diag: {dv_diag[:5]}, de_diag: {de_diag[:5]}")
            return torch.eye(batch_size, device=device)

        I = torch.eye(batch_size, device=device)
        L = I - Theta

        return L

    def _compute_degree_matrices(self, H):
        d_v = H.sum(dim=1)
        d_e = H.sum(dim=0)

        d_v = torch.clamp(d_v, min=1e-8)
        d_e = torch.clamp(d_e, min=1e-8)

        Dv = torch.diag(d_v)
        De = torch.diag(d_e)

        return Dv, De


class SimpleAutoencoder(nn.Module):
    """ç®€å•çš„è‡ªç¼–ç å™¨ç”¨äºå¼‚å¸¸æ£€æµ‹åŸºçº¿æ¯”è¾ƒ"""
    
    def __init__(self, input_dim, hidden_dims=[256, 128, 64, 32]):
        super(SimpleAutoencoder, self).__init__()
        
        # ç¼–ç å™¨
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim
        self.encoder = nn.Sequential(*encoder_layers)
        
        # è§£ç å™¨
        decoder_layers = []
        hidden_dims_reversed = list(reversed(hidden_dims[:-1])) + [input_dim]
        for hidden_dim in hidden_dims_reversed:
            decoder_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU() if hidden_dim != input_dim else nn.Identity()
            ])
            prev_dim = hidden_dim
        self.decoder = nn.Sequential(*decoder_layers)
        
    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded, encoded
    
    def get_reconstruction_error(self, x):
        """è®¡ç®—é‡æ„è¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°"""
        with torch.no_grad():
            reconstructed, _ = self.forward(x)
            error = torch.mean((x - reconstructed) ** 2, dim=1)
            return error


class BaselineModelTrainer:
    """åŸºçº¿æ¨¡å‹è®­ç»ƒå™¨ - æ”¯æŒAutoencoderã€SVMã€Isolation Forest"""
    
    def __init__(self, config, device='cuda'):
        self.config = config
        self.device = device
        self.data_generator = ComplexDataGenerator(config)
        
    def train_autoencoder(self, dataset='bloodmnist', epochs=50, batch_size=32):
        """è®­ç»ƒè‡ªç¼–ç å™¨åŸºçº¿æ¨¡å‹"""
        print(f"ğŸ”§ å¼€å§‹è®­ç»ƒè‡ªç¼–ç å™¨åŸºçº¿æ¨¡å‹ - {dataset.upper()}")
        
        # è·å–æ•°æ®ç‰¹å¾ç»´åº¦
        sample_images, sample_text, _, _ = self.data_generator.generate_multimodal_data(
            batch_size, self.device, split='train', dataset=dataset, anomaly_ratio=0.0
        )
        
        # è®¡ç®—è¾“å…¥ç‰¹å¾ç»´åº¦
        img_features = sample_images.view(sample_images.shape[0], -1)
        text_features = sample_text.float().mean(dim=1, keepdim=True).repeat(1, 64)  # ç®€åŒ–æ–‡æœ¬ç‰¹å¾
        combined_features = torch.cat([img_features, text_features], dim=1)
        input_dim = combined_features.shape[1]
        
        # åˆ›å»ºè‡ªç¼–ç å™¨æ¨¡å‹
        autoencoder = SimpleAutoencoder(input_dim).to(self.device)
        optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå¾ªç¯
        autoencoder.train()
        for epoch in range(epochs):
            epoch_losses = []
            
            for batch_idx in range(15):  # 15ä¸ªè®­ç»ƒæ‰¹æ¬¡
                try:
                    # è·å–æ­£å¸¸æ•°æ®è¿›è¡Œè®­ç»ƒ
                    images, text_sequences, labels, _ = \
                        self.data_generator.generate_multimodal_data(
                            batch_size, self.device, split='train', dataset=dataset, anomaly_ratio=0.0
                        )
                    
                    # ç‰¹å¾æå–å’Œç»„åˆ - ç¡®ä¿æ¢¯åº¦ä¼ æ’­
                    img_features = images.view(images.shape[0], -1)
                    text_features = text_sequences.float().mean(dim=1, keepdim=True).repeat(1, 64)
                    combined_features = torch.cat([img_features, text_features], dim=1)
                    
                    # ç¡®ä¿å¼ é‡éœ€è¦æ¢¯åº¦
                    combined_features = combined_features.detach().requires_grad_(True)
                    
                    optimizer.zero_grad()
                    reconstructed, _ = autoencoder(combined_features)
                    loss = criterion(reconstructed, combined_features.detach())  # ç›®æ ‡ä¸éœ€è¦æ¢¯åº¦
                    loss.backward()
                    optimizer.step()
                    
                    epoch_losses.append(loss.item())
                    
                except Exception as e:
                    print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
            
            if epoch % 10 == 0 and epoch_losses:
                avg_loss = np.mean(epoch_losses)
                print(f"Epoch {epoch}/{epochs}, Avg Loss: {avg_loss:.4f}")
        
        # è¯„ä¼°
        return self._evaluate_autoencoder(autoencoder, dataset, batch_size)
    
    def train_svm(self, dataset='bloodmnist', batch_size=32, nu=0.1):
        """è®­ç»ƒOne-Class SVMåŸºçº¿æ¨¡å‹"""
        print(f"ğŸ”§ å¼€å§‹è®­ç»ƒOne-Class SVMåŸºçº¿æ¨¡å‹ - {dataset.upper()}")
        
        # æ”¶é›†è®­ç»ƒæ•°æ®
        all_features = []
        for batch_idx in range(20):  # æ”¶é›†æ›´å¤šæ•°æ®ç”¨äºè®­ç»ƒ
            images, text_sequences, labels, _ = \
                self.data_generator.generate_multimodal_data(
                    batch_size, self.device, split='train', dataset=dataset, anomaly_ratio=0.0
                )
            
            # ç‰¹å¾æå–
            img_features = images.view(images.shape[0], -1).cpu().numpy()
            text_features = text_sequences.float().mean(dim=1, keepdim=True).repeat(1, 64).cpu().numpy()
            combined_features = np.concatenate([img_features, text_features], axis=1)
            all_features.append(combined_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        X_train = np.vstack(all_features)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # è®­ç»ƒOne-Class SVM
        svm_model = OneClassSVM(nu=nu, kernel='rbf', gamma='scale')
        svm_model.fit(X_train_scaled)
        
        # è¯„ä¼°
        return self._evaluate_svm(svm_model, scaler, dataset, batch_size)
    
    def train_isolation_forest(self, dataset='bloodmnist', batch_size=32, contamination=0.1):
        """è®­ç»ƒIsolation ForeståŸºçº¿æ¨¡å‹"""
        print(f"ğŸ”§ å¼€å§‹è®­ç»ƒIsolation ForeståŸºçº¿æ¨¡å‹ - {dataset.upper()}")
        
        # æ”¶é›†è®­ç»ƒæ•°æ®
        all_features = []
        for batch_idx in range(20):  # æ”¶é›†æ›´å¤šæ•°æ®ç”¨äºè®­ç»ƒ
            images, text_sequences, labels, _ = \
                self.data_generator.generate_multimodal_data(
                    batch_size, self.device, split='train', dataset=dataset, anomaly_ratio=0.0
                )
            
            # ç‰¹å¾æå–
            img_features = images.view(images.shape[0], -1).cpu().numpy()
            text_features = text_sequences.float().mean(dim=1, keepdim=True).repeat(1, 64).cpu().numpy()
            combined_features = np.concatenate([img_features, text_features], axis=1)
            all_features.append(combined_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        X_train = np.vstack(all_features)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # è®­ç»ƒIsolation Forest
        iso_forest = IsolationForest(contamination=contamination, random_state=42, n_estimators=100)
        iso_forest.fit(X_train_scaled)
        
        # è¯„ä¼°
        return self._evaluate_isolation_forest(iso_forest, scaler, dataset, batch_size)
    
    def _evaluate_autoencoder(self, autoencoder, dataset, batch_size):
        """è¯„ä¼°è‡ªç¼–ç å™¨æ¨¡å‹"""
        autoencoder.eval()
        all_scores = []
        all_labels = []
        
        with torch.no_grad():
            for batch_idx in range(10):  # æµ‹è¯•æ‰¹æ¬¡
                try:
                    images, text_sequences, labels, _ = \
                        self.data_generator.generate_multimodal_data(
                            batch_size, self.device, split='test', dataset=dataset, anomaly_ratio=0.2
                        )
                    
                    # ç‰¹å¾æå–
                    img_features = images.view(images.shape[0], -1)
                    text_features = text_sequences.float().mean(dim=1, keepdim=True).repeat(1, 64)
                    combined_features = torch.cat([img_features, text_features], dim=1)
                    
                    # è·å–é‡æ„è¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°
                    reconstruction_errors = autoencoder.get_reconstruction_error(combined_features)
                    
                    all_scores.extend(reconstruction_errors.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
                    
                except Exception as e:
                    print(f"è¯„ä¼°æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                    continue
        
        if len(all_scores) == 0:
            print("âš ï¸ è‡ªç¼–ç å™¨è¯„ä¼°å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ‰¹æ¬¡")
            return {'auc': 0.0, 'accuracy': 0.0, 'error': 'No valid test batches'}
        
        return self._compute_metrics(all_scores, all_labels)
    
    def _evaluate_svm(self, svm_model, scaler, dataset, batch_size):
        """è¯„ä¼°SVMæ¨¡å‹"""
        all_scores = []
        all_labels = []
        
        for batch_idx in range(10):  # æµ‹è¯•æ‰¹æ¬¡
            try:
                images, text_sequences, labels, _ = \
                    self.data_generator.generate_multimodal_data(
                        batch_size, self.device, split='test', dataset=dataset, anomaly_ratio=0.2
                    )
                
                # ç‰¹å¾æå–
                img_features = images.view(images.shape[0], -1).cpu().numpy()
                text_features = text_sequences.float().mean(dim=1, keepdim=True).repeat(1, 64).cpu().numpy()
                combined_features = np.concatenate([img_features, text_features], axis=1)
                
                # æ ‡å‡†åŒ–
                combined_features_scaled = scaler.transform(combined_features)
                
                # è·å–å†³ç­–åˆ†æ•°ï¼ˆè·ç¦»åˆ†ç¦»è¶…å¹³é¢çš„è·ç¦»ï¼‰
                decision_scores = svm_model.decision_function(combined_features_scaled)
                # è½¬æ¢ä¸ºå¼‚å¸¸åˆ†æ•°ï¼ˆè´Ÿå€¼è¡¨ç¤ºå¼‚å¸¸ï¼‰
                anomaly_scores = -decision_scores  # è´Ÿå€¼è¶Šå¤§è¶Šå¼‚å¸¸
                
                all_scores.extend(anomaly_scores)
                all_labels.extend(labels.cpu().numpy())
                
            except Exception as e:
                print(f"SVMè¯„ä¼°æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                continue
        
        if len(all_scores) == 0:
            print("âš ï¸ SVMè¯„ä¼°å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ‰¹æ¬¡")
            return {'auc': 0.0, 'accuracy': 0.0, 'error': 'No valid test batches'}
        
        return self._compute_metrics(all_scores, all_labels)
    
    def _evaluate_isolation_forest(self, iso_forest, scaler, dataset, batch_size):
        """è¯„ä¼°Isolation Forestæ¨¡å‹"""
        all_scores = []
        all_labels = []
        
        for batch_idx in range(10):  # æµ‹è¯•æ‰¹æ¬¡
            try:
                images, text_sequences, labels, _ = \
                    self.data_generator.generate_multimodal_data(
                        batch_size, self.device, split='test', dataset=dataset, anomaly_ratio=0.2
                    )
                
                # ç‰¹å¾æå–
                img_features = images.view(images.shape[0], -1).cpu().numpy()
                text_features = text_sequences.float().mean(dim=1, keepdim=True).repeat(1, 64).cpu().numpy()
                combined_features = np.concatenate([img_features, text_features], axis=1)
                
                # æ ‡å‡†åŒ–
                combined_features_scaled = scaler.transform(combined_features)
                
                # è·å–å¼‚å¸¸åˆ†æ•°
                anomaly_scores = iso_forest.decision_function(combined_features_scaled)
                # è½¬æ¢ä¸ºæ­£å€¼ï¼ˆå€¼è¶Šå¤§è¶Šæ­£å¸¸ï¼Œæˆ‘ä»¬éœ€è¦åè½¬ï¼‰
                anomaly_scores = -anomaly_scores  # è´Ÿå€¼è¶Šå¤§è¶Šå¼‚å¸¸
                
                all_scores.extend(anomaly_scores)
                all_labels.extend(labels.cpu().numpy())
                
            except Exception as e:
                print(f"Isolation Forestè¯„ä¼°æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                continue
        
        if len(all_scores) == 0:
            print("âš ï¸ Isolation Forestè¯„ä¼°å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ‰¹æ¬¡")
            return {'auc': 0.0, 'accuracy': 0.0, 'error': 'No valid test batches'}
        
        return self._compute_metrics(all_scores, all_labels)
    
    def _compute_metrics(self, all_scores, all_labels):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„é€šç”¨æ–¹æ³•"""
        all_scores = np.array(all_scores)
        all_labels = np.array(all_labels)
        
        results = {}
        
        try:
            results['auc'] = roc_auc_score(all_labels, all_scores)
        except:
            results['auc'] = 0.5

        try:
            from sklearn.metrics import accuracy_score, precision_recall_curve
            
            threshold = np.median(all_scores)  # ä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºé˜ˆå€¼
            predicted_labels = (all_scores > threshold).astype(int)
            results['accuracy'] = accuracy_score(all_labels, predicted_labels)

        except Exception as e:
            print(f"Warning: Could not calculate accuracy: {e}")
            results['accuracy'] = 0.5

        return results


# ä¿®æ”¹åçš„å¤šæ•°æ®é›†åŠ è½½å™¨ - æ”¯æŒBloodMNISTã€PathMNISTã€OrganaMNIST
class MultiDatasetLoader:
    """å¤šæ•°æ®é›†åŠ è½½å™¨ - æ”¯æŒä»F:\Desktop\bloodmnist\DataåŠ è½½ä¸‰ä¸ªæ•°æ®é›†"""

    def __init__(self, data_root="F:\\Desktop\\bloodmnist\\Data", batch_size=128, cache_size=1024, 
                 embedding_dim=64, max_seq_len=64, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
        self.data_root = data_root
        self.batch_size = batch_size
        self.cache_size = cache_size
        self.embedding_dim = embedding_dim
        self.max_seq_len = max_seq_len
        
        # é™æ€æ•°æ®åˆ’åˆ†å‚æ•°
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio  
        self.test_ratio = test_ratio
        
        # ç¡®ä¿æ¯”ä¾‹æ€»å’Œä¸º1
        total_ratio = train_ratio + val_ratio + test_ratio
        if abs(total_ratio - 1.0) > 1e-6:
            raise ValueError(f"æ•°æ®åˆ’åˆ†æ¯”ä¾‹æ€»å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º: {total_ratio}")

        # æ•°æ®é›†é…ç½®
        self.dataset_configs = {
            'bloodmnist': {
                'path': os.path.join(data_root, 'bloodmnist'),
                'classes': ["basophil", "eosinophil", "erythroblast", "ig", 
                           "lymphocyte", "monocyte", "neutrophil", "platelet"],
                'num_classes': 8
            },
            'pathmnist': {
                'path': os.path.join(data_root, 'pathmnist'),
                'classes': ["adipose", "background", "debris", "lymphocytes", "mucus",
                           "smooth_muscle", "normal_colon_mucosa", "cancer_epithelium", "colorectal_adenocarcinoma"],
                'num_classes': 9
            },
            'organamnist': {
                'path': os.path.join(data_root, 'organamnist'), 
                'classes': ["bladder", "femur", "heart", "kidney", "breast", "lung",
                           "ovary", "pancreas", "prostate", "spleen", "tibia"],
                'num_classes': 11
            }
        }

        # å­˜å‚¨æ‰€æœ‰æ•°æ®é›†çš„æ•°æ®
        self.datasets = {}
        
        # è¯æ±‡è¡¨ç›¸å…³ï¼ˆè·¨æ•°æ®é›†å…±äº«ï¼‰
        self.vocab = {}
        self.idx_to_word = {}
        self.word_to_idx = {'<pad>': 0, '<unk>': 1}
        self.vocab_size = 2

        self._initialize_all_datasets()

    def _initialize_all_datasets(self):
        """åˆå§‹åŒ–æ‰€æœ‰ä¸‰ä¸ªæ•°æ®é›†"""
        print(f"ğŸ”„ æ­£åœ¨ä» {self.data_root} åŠ è½½ä¸‰ä¸ªæ•°æ®é›†...")
        
        # é¦–å…ˆæ„å»ºè·¨æ•°æ®é›†çš„è¯æ±‡è¡¨
        self._build_global_vocabulary()
        
        # ç„¶ååŠ è½½æ¯ä¸ªæ•°æ®é›†
        for dataset_name, config in self.dataset_configs.items():
            print(f"\nğŸ“‚ æ­£åœ¨åŠ è½½ {dataset_name.upper()} æ•°æ®é›†...")
            self._load_single_dataset(dataset_name, config)
            
        print(f"\nâœ… æ‰€æœ‰æ•°æ®é›†åŠ è½½å®Œæˆ!")
        print(f"   ğŸ“Š BloodMNIST: {len(self.datasets['bloodmnist']['images'])} æ ·æœ¬")
        print(f"   ğŸ“Š PathMNIST: {len(self.datasets['pathmnist']['images'])} æ ·æœ¬") 
        print(f"   ğŸ“Š OrganaMNIST: {len(self.datasets['organamnist']['images'])} æ ·æœ¬")
        print(f"   ğŸ“ å…¨å±€è¯æ±‡è¡¨å¤§å°: {self.vocab_size}")

    def _build_global_vocabulary(self):
        """æ„å»ºè·¨æ‰€æœ‰æ•°æ®é›†çš„å…¨å±€è¯æ±‡è¡¨"""
        print("ğŸ”¤ æ„å»ºå…¨å±€è¯æ±‡è¡¨...")
        all_tokens = []
        
        for dataset_name, config in self.dataset_configs.items():
            text_path = os.path.join(config['path'], f"{dataset_name}_text_descriptions.json")
            if os.path.exists(text_path):
                with open(text_path, 'r', encoding='utf-8') as f:
                    text_data = json.load(f)
                
                samples_data = text_data.get('data', text_data.get('samples', []))
                for sample in samples_data:
                    description = sample.get('text_description', sample.get('description', ''))
                    tokens = [word.strip('.,!?;:"\'').lower() for word in jieba.lcut(description) 
                             if word.strip('.,!?;:"\'').isalnum()]
                    all_tokens.extend(tokens)
        
        # æ„å»ºè¯æ±‡è¡¨
        unique_tokens = sorted(list(set(all_tokens)))
        for word in unique_tokens:
            if word not in self.word_to_idx:
                self.word_to_idx[word] = self.vocab_size
                self.vocab_size += 1
        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        
        print(f"   âœ… å…¨å±€è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œè¯æ±‡é‡: {self.vocab_size}")

    def _load_single_dataset(self, dataset_name, config):
        """åŠ è½½å•ä¸ªæ•°æ®é›†"""
        dataset_path = config['path']
        
        # åŠ è½½å›¾åƒæ•°æ®
        images_path = os.path.join(dataset_path, f"{dataset_name}_images.pkl")
        with open(images_path, 'rb') as f:
            image_data = pickle.load(f)
        
        # åŠ è½½æ–‡æœ¬æ•°æ®
        text_path = os.path.join(dataset_path, f"{dataset_name}_text_descriptions.json")
        with open(text_path, 'r', encoding='utf-8') as f:
            text_data = json.load(f)
        
        # å¤„ç†å›¾åƒå’Œæ ‡ç­¾
        original_images = torch.from_numpy(np.array(image_data['images'])).float()
        original_labels = torch.from_numpy(np.array(image_data['labels'])).long()
        
        # å¤„ç†æ–‡æœ¬æ•°æ®
        raw_text_descriptions = []
        samples_data = text_data.get('data', text_data.get('samples', []))
        for sample in samples_data:
            description = sample.get('text_description', sample.get('description', ''))
            raw_text_descriptions.append(description)
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—ï¼ˆä½¿ç”¨å…¨å±€è¯æ±‡è¡¨ï¼‰
        text_sequences = []
        for desc in raw_text_descriptions:
            tokens = [word.strip('.,!?;:"\'').lower() for word in jieba.lcut(desc) 
                     if word.strip('.,!?;:"\'').isalnum()]
            indexed_tokens = [self.word_to_idx.get(token, self.word_to_idx['<unk>']) for token in tokens]
            
            if len(indexed_tokens) < self.max_seq_len:
                padded_sequence = indexed_tokens + [self.word_to_idx['<pad>']] * (
                            self.max_seq_len - len(indexed_tokens))
            else:
                padded_sequence = indexed_tokens[:self.max_seq_len]
            text_sequences.append(padded_sequence)
        
        original_text_sequences = torch.tensor(text_sequences, dtype=torch.long)
        
        # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
        min_samples = min(len(original_images), len(original_text_sequences), len(original_labels))
        original_images = original_images[:min_samples]
        original_text_sequences = original_text_sequences[:min_samples]
        original_labels = original_labels[:min_samples]
        
        # æ•°æ®é¢„å¤„ç†
        if original_images.max() > 1.0:
            original_images = original_images / 255.0
        original_images = (original_images - 0.5) / 0.5
        
        # æ‰§è¡Œé™æ€æ•°æ®åˆ’åˆ†
        total_samples = min_samples
        indices = np.random.permutation(total_samples)
        
        train_end = int(total_samples * self.train_ratio)
        val_end = train_end + int(total_samples * self.val_ratio)
        
        train_indices = indices[:train_end]
        val_indices = indices[train_end:val_end]
        test_indices = indices[val_end:]
        
        # å­˜å‚¨æ•°æ®é›†
        self.datasets[dataset_name] = {
            'images': original_images,
            'text_sequences': original_text_sequences,
            'labels': original_labels,
            'train_indices': train_indices,
            'val_indices': val_indices,
            'test_indices': test_indices,
            'class_names': config['classes'],
            'num_classes': config['num_classes']
        }
        
        print(f"   âœ… {dataset_name.upper()} åŠ è½½å®Œæˆ:")
        print(f"      ğŸŸ¢ è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬")
        print(f"      ğŸŸ¡ éªŒè¯é›†: {len(val_indices)} æ ·æœ¬")
        print(f"      ğŸ”´ æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬")

    def get_split_batch(self, batch_size, device, split='train', dataset='bloodmnist'):
        """
        ä»æŒ‡å®šæ•°æ®é›†çš„æŒ‡å®šåˆ’åˆ†è·å–æ‰¹æ¬¡æ•°æ®
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
            split: æ•°æ®åˆ’åˆ† {'train', 'val', 'test'}
            dataset: æ•°æ®é›†åç§° {'bloodmnist', 'pathmnist', 'organamnist'}
        """
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset}")
        
        dataset_data = self.datasets[dataset]
        
        # æ ¹æ®åˆ’åˆ†é€‰æ‹©å¯¹åº”çš„ç´¢å¼•
        if split == 'train':
            available_indices = dataset_data['train_indices']
            split_name = "ğŸŸ¢ TRAIN"
        elif split == 'val':
            available_indices = dataset_data['val_indices']
            split_name = "ğŸŸ¡ VAL"
        elif split == 'test':
            available_indices = dataset_data['test_indices']
            split_name = "ğŸ”´ TEST"
        else:
            raise ValueError(f"æ— æ•ˆçš„æ•°æ®åˆ’åˆ†: {split}")
        
        if len(available_indices) == 0:
            raise ValueError(f"{dataset.upper()} {split_name} åˆ’åˆ†ä¸ºç©º!")
        
        # ä»å¯¹åº”åˆ’åˆ†ä¸­éšæœºé‡‡æ ·
        if batch_size > len(available_indices):
            selected_indices = np.random.choice(available_indices, batch_size, replace=True)
        else:
            selected_indices = np.random.choice(available_indices, batch_size, replace=False)
        
        # è·å–æ•°æ®
        images = dataset_data['images'][selected_indices].to(device)
        text_sequences = dataset_data['text_sequences'][selected_indices].to(device)
        labels = dataset_data['labels'][selected_indices].to(device)
        
        return images, text_sequences, labels

    def get_dataset_info(self, dataset='bloodmnist'):
        """è·å–æŒ‡å®šæ•°æ®é›†çš„ä¿¡æ¯"""
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥æ•°æ®é›†: {dataset}")
        
        dataset_data = self.datasets[dataset]
        return {
            'num_classes': dataset_data['num_classes'],
            'class_names': dataset_data['class_names'],
            'total_samples': len(dataset_data['images']),
            'train_samples': len(dataset_data['train_indices']),
            'val_samples': len(dataset_data['val_indices']),
            'test_samples': len(dataset_data['test_indices'])
        }


class ComplexDataGenerator:
    """å¤æ‚æ•°æ®ç”Ÿæˆå™¨ - æ”¯æŒå¤šæ•°æ®é›†"""

    def __init__(self, config):
        self.config = config
        self.anomaly_generator = ComplexAnomalyGenerator()
        self.multi_loader = MultiDatasetLoader(
            data_root=config.get('data_root', "F:\\Desktop\\bloodmnist\\Data"),
            batch_size=config.get('batch_size', 32),
            cache_size=config.get('cache_size', 1024),
            embedding_dim=config.get('embedding_dim', 64),
            max_seq_len=config.get('max_seq_len', 64)
        )

    def generate_multimodal_data(self, batch_size, device, split='train', dataset='bloodmnist', anomaly_ratio=0.15):
        """
        âœ… æ ¹æ®æ•°æ®åˆ’åˆ†å’Œæ•°æ®é›†ç”Ÿæˆå¤šæ¨¡æ€æ•°æ®
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡
            split: æ•°æ®åˆ’åˆ† {'train', 'val', 'test'}
            dataset: æ•°æ®é›†åç§° {'bloodmnist', 'pathmnist', 'organamnist'}
            anomaly_ratio: å¼‚å¸¸æ ·æœ¬æ¯”ä¾‹
        """
        # ä½¿ç”¨æ–°çš„å¤šæ•°æ®é›†æ¥å£è·å–æ•°æ®
        images, text_sequences, original_labels = self.multi_loader.get_split_batch(
            batch_size, device, split, dataset)

        # æ·»åŠ åŸºç¡€å™ªå£°
        base_noise = torch.randn_like(images) * 0.05
        images = images + base_noise

        # ç”Ÿæˆå¼‚å¸¸æ ·æœ¬
        images, perturbed_text_sequences, anomaly_labels, anomaly_types = \
            self.anomaly_generator.generate_anomalies_with_real_text(
                images, text_sequences, original_labels, self.config, anomaly_ratio,
                vocab_size=self.multi_loader.vocab_size
            )

        return images, perturbed_text_sequences, anomaly_labels, anomaly_types


        return images, perturbed_text_sequences, anomaly_labels, anomaly_types

    def _apply_data_augmentation(self, images):  # æ­¤æ–¹æ³•å·²åœ¨ ComplexAnomalyGenerator å†…éƒ¨è°ƒç”¨ï¼Œè¿™é‡Œä¿ç•™ä½†æ³¨æ„ä¸è¦é‡å¤è°ƒç”¨
        batch_size = images.shape[0]

        for i in range(batch_size):
            if np.random.random() > 0.5:
                brightness_factor = 0.8 + np.random.random() * 0.4
                images[i] = images[i] * brightness_factor

            if np.random.random() > 0.6:
                contrast_factor = 0.7 + np.random.random() * 0.6
                mean_val = images[i].mean()
                images[i] = (images[
                                 i] - mean_val) * contrast_factor + mean_val  # Corrected var name `contrast_val` to `contrast_factor`

        images = torch.clamp(images, -2, 2)
        return images

    def _add_text_noise(self, text_sequences):
        return text_sequences


class ComplexAnomalyGenerator:
    """ç”Ÿæˆæ›´å¤æ‚å’ŒçœŸå®çš„å¼‚å¸¸æ¨¡å¼ - è°ƒæ•´ä»¥é€‚åº”åºåˆ—IDæˆ–åŸå§‹æ–‡æœ¬"""

    def __init__(self, anomaly_types=['structural', 'contextual', 'collective', 'multimodal', 'boundary']):
        self.anomaly_types = anomaly_types

    def generate_anomalies_with_real_text(self, images, real_text_sequences, labels, config, anomaly_ratio=0.15,
                                          vocab_size=None):  # æ–°å¢ vocab_size
        num_samples = len(images)
        num_anomalies = int(num_samples * anomaly_ratio)
        device = images.device

        anomaly_labels = torch.zeros(num_samples, device=device)
        anomaly_indices = torch.randperm(num_samples)[:num_anomalies]
        anomaly_labels[anomaly_indices] = 1

        anomaly_type_assignment = np.random.choice(self.anomaly_types, num_anomalies)

        modified_text_sequences = real_text_sequences.clone()

        normal_indices = torch.where(anomaly_labels == 0)[0]

        for i, idx in enumerate(anomaly_indices):
            anomaly_type = anomaly_type_assignment[i]

            if anomaly_type == 'structural':
                images[idx] = self._apply_structural_anomaly(images[idx])
                modified_text_sequences[idx] = self._add_sequence_anomaly(modified_text_sequences[idx], 'structural',
                                                                          vocab_size, device)

            elif anomaly_type == 'contextual':
                if len(normal_indices) > 0:
                    rand_idx = torch.randint(0, len(normal_indices), (1,))
                    random_normal_idx = normal_indices[rand_idx.item()]
                    modified_text_sequences[idx] = real_text_sequences[random_normal_idx].clone()
                    modified_text_sequences[idx] = self._add_sequence_anomaly(modified_text_sequences[idx],
                                                                              'contextual', vocab_size, device)

            elif anomaly_type == 'collective':
                images[idx] = self._apply_collective_anomaly(images[idx])
                modified_text_sequences[idx] = self._add_sequence_anomaly(modified_text_sequences[idx], 'collective',
                                                                          vocab_size, device)

            elif anomaly_type == 'multimodal':
                images[idx] = self._apply_multimodal_anomaly(images[idx])
                modified_text_sequences[idx] = self._add_sequence_anomaly(modified_text_sequences[idx], 'multimodal',
                                                                          vocab_size, device)

            elif anomaly_type == 'boundary':
                if len(normal_indices) > 1:
                    rand_idx = torch.randint(0, len(normal_indices), (1,))
                    other_normal_idx = normal_indices[rand_idx.item()]
                    alpha = 0.7 + torch.rand(1).item() * 0.2

                    images[idx] = alpha * images[idx] + (1 - alpha) * images[other_normal_idx]
                    modified_text_sequences[idx] = (alpha * real_text_sequences[idx].float() + \
                                                    (1 - alpha) * real_text_sequences[
                                                        other_normal_idx].float()).round().long()

        full_anomaly_types = np.array(['normal'] * num_samples)
        full_anomaly_types[anomaly_indices.cpu().numpy()] = anomaly_type_assignment

        return images, modified_text_sequences, anomaly_labels, full_anomaly_types

    def _add_sequence_anomaly(self, sequence_ids, anomaly_type, vocab_size, device):
        """å¯¹æ–‡æœ¬IDåºåˆ—æ·»åŠ ç®€å•å¼‚å¸¸æ‰°åŠ¨ (å·²ç¡®ä¿ vocab_size ä¼ å…¥)"""
        seq_len = sequence_ids.shape[0]

        if vocab_size is None or vocab_size < 2:
            return sequence_ids

        if anomaly_type == 'structural':
            num_to_perturb = int(seq_len * 0.1)
            if num_to_perturb == 0 and seq_len > 0: num_to_perturb = 1

            perturb_positions = torch.randperm(seq_len)[:num_to_perturb]
            random_word_ids = torch.randint(2, vocab_size, (num_to_perturb,), device=device, dtype=torch.long)

            for i, pos in enumerate(perturb_positions):
                sequence_ids[pos] = random_word_ids[i]

        elif anomaly_type == 'contextual':
            if seq_len > 0 and np.random.random() > 0.5:
                insert_pos = torch.randint(0, seq_len, (1,)).item()
                if np.random.random() > 0.5:
                    new_ids = torch.cat((sequence_ids[:insert_pos], torch.tensor([1], device=device, dtype=torch.long),
                                         sequence_ids[insert_pos:]))
                else:
                    if insert_pos < seq_len and sequence_ids[insert_pos] != 0:
                        new_ids = torch.cat((sequence_ids[:insert_pos], sequence_ids[insert_pos + 1:]))
                    else:
                        new_ids = sequence_ids

                if len(new_ids) < seq_len:
                    sequence_ids = torch.cat(
                        (new_ids, torch.tensor([0] * (seq_len - len(new_ids)), device=device, dtype=torch.long)))
                elif len(new_ids) > seq_len:
                    sequence_ids = new_ids[:seq_len]
                else:
                    sequence_ids = new_ids

        elif anomaly_type == 'collective':
            if seq_len > 5:
                start_pos = torch.randint(0, seq_len - 5, (1,)).item()
                pattern_id = torch.randint(2, vocab_size, (1,)).item()
                sequence_ids[start_pos:start_pos + 5] = pattern_id

        elif anomaly_type == 'multimodal':
            if seq_len > 0:
                pos = torch.randint(0, seq_len, (1,)).item()
                sequence_ids[pos] = torch.randint(2, vocab_size, (1,)).item()
                if seq_len < 2 * sequence_ids.shape[0] and np.random.random() > 0.5:
                    insert_pos = torch.randint(0, seq_len, (1,)).item()
                    new_ids = torch.cat((sequence_ids[:insert_pos], torch.tensor([1], device=device, dtype=torch.long),
                                         sequence_ids[insert_pos:]))
                    if len(new_ids) > seq_len:
                        sequence_ids = new_ids[:seq_len]
                    else:
                        sequence_ids = new_ids

        return sequence_ids

    def _apply_structural_anomaly(self, image):
        anomaly_type = np.random.choice(['occlusion', 'noise', 'blur'])

        if anomaly_type == 'occlusion':
            h, w = image.shape[-2:]
            mask_h, mask_w = h // 4, w // 4
            start_h = torch.randint(0, h - mask_h, (1,)).item()
            start_w = torch.randint(0, w - mask_w, (1,)).item()
            image[:, start_h:start_h + mask_h, start_w:start_w + mask_w] = 0

        elif anomaly_type == 'noise':
            noise = torch.randn_like(image) * 0.5
            image = image + noise

        elif anomaly_type == 'blur':
            kernel_size = 5
            kernel = torch.ones(1, 1, kernel_size, kernel_size, device=image.device) / (kernel_size * kernel_size)

            if len(image.shape) == 3:
                img_batch = image.unsqueeze(0)
            else:
                img_batch = image

            blurred_channels = []
            for c in range(img_batch.shape[1]):
                single_channel = img_batch[:, c:c + 1, :, :]
                padded = F.pad(single_channel, (kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2),
                               mode='reflect')
                blurred = F.conv2d(padded, kernel, padding=0)
                blurred_channels.append(blurred)

            blurred_image = torch.cat(blurred_channels, dim=1)

            if len(image.shape) == 3:
                image = blurred_image.squeeze(0)
            else:
                image = blurred_image

        return image

    def _apply_collective_anomaly(self, image):
        h, w = image.shape[-2:]
        pattern_type = np.random.choice(['checkerboard', 'stripe', 'spot'])

        if pattern_type == 'checkerboard':
            for i in range(0, h, 4):
                for j in range(0, w, 4):
                    if (i // 4 + j // 4) % 2 == 0:
                        image[:, i:i + 4, j:j + 4] *= 0.3

        elif pattern_type == 'stripe':
            for i in range(0, h, 3):
                image[:, i, :] *= 0.5

        elif pattern_type == 'spot':
            num_spots = 5
            for _ in range(num_spots):
                cy = torch.randint(5, h - 5, (1,)).item()
                cx = torch.randint(5, w - 5, (1,)).item()
                image[:, cy - 2:cy + 2, cx - 2:cx + 2] = 1.0

        return image

    def _apply_multimodal_anomaly(self, image):
        image = self._apply_structural_anomaly(image)
        if image.shape[0] == 3:
            channel_shift = torch.randn(3, 1, 1, device=image.device) * 0.3
            image = image + channel_shift
        return image


class AblationTrainer:
    """æ¶ˆèå®éªŒè®­ç»ƒå™¨"""

    def __init__(self, model, config, device='cuda', dataset='bloodmnist'):
        self.model = model
        self.config = config
        self.device = device
        self.dataset = dataset  # æ–°å¢ï¼šå½“å‰ä½¿ç”¨çš„æ•°æ®é›†
        self.data_generator = ComplexDataGenerator(config)

        self.optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config.get('learning_rate', 0.001),
            weight_decay=config.get('weight_decay', 1e-5)
        )
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=10
        )

    def train(self, num_epochs=50, train_batches=20, val_batches=5):
        """
        âœ… è®­ç»ƒæ¨¡å‹ï¼ˆä¸¥æ ¼çš„æ•°æ®åˆ’åˆ†ï¼‰
        - è®­ç»ƒé˜¶æ®µï¼šä»…ä½¿ç”¨ Train æ•°æ®è¿›è¡Œæ¢¯åº¦æ›´æ–°
        - éªŒè¯é˜¶æ®µï¼šä»…ä½¿ç”¨ Val æ•°æ®è¿›è¡Œæ¨¡å‹é€‰æ‹©å’Œæ—©åœ
        - æµ‹è¯•é˜¶æ®µï¼šå®Œå…¨éš”ç¦»ï¼Œä¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¥è§¦
        """
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - ä¸¥æ ¼çš„æ•°æ®åˆ’åˆ†æ¨¡å¼")
        print(f"   ğŸŸ¢ è®­ç»ƒæ•°æ®ï¼šä»…ç”¨äºæ¢¯åº¦æ›´æ–°")
        print(f"   ğŸŸ¡ éªŒè¯æ•°æ®ï¼šä»…ç”¨äºæ¨¡å‹é€‰æ‹©å’Œæ—©åœ")
        print(f"   ğŸ”´ æµ‹è¯•æ•°æ®ï¼šå®Œå…¨éš”ç¦»ï¼Œè®­ç»ƒæœŸé—´ä¸æ¥è§¦")
        
        self.model.train()
        train_losses = []
        val_metrics = []
        batch_size = self.config.get('batch_size', 16)

        for epoch in range(num_epochs):
            self.model.train()
            epoch_losses = []

            # âœ… è®­ç»ƒé˜¶æ®µï¼šä»…ä½¿ç”¨ Train æ•°æ®
            for batch_idx in range(train_batches):
                torch.set_grad_enabled(True)
                self.model.train()

                # ğŸŸ¢ ä¸¥æ ¼ä½¿ç”¨è®­ç»ƒé›†æ•°æ®ï¼ŒæŒ‡å®šæ•°æ®é›†
                images, text_sequences, labels, anomaly_types = \
                    self.data_generator.generate_multimodal_data(
                        batch_size, self.device, split='train', dataset=self.dataset, anomaly_ratio=0.15
                    )

                self.optimizer.zero_grad()
                _, losses = self.model(images, text_sequences, labels)

                losses['total_loss'].backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

                epoch_losses.append(losses['total_loss'].item() if losses['total_loss'].numel() == 1 else losses[
                    'total_loss'].mean().item())

            avg_train_loss = np.mean(epoch_losses)
            train_losses.append(avg_train_loss)

            # âœ… éªŒè¯é˜¶æ®µï¼šä»…ä½¿ç”¨ Val æ•°æ®ï¼ˆç”¨äºæ—©åœå’Œæ¨¡å‹é€‰æ‹©ï¼‰
            torch.set_grad_enabled(False)
            if epoch % 10 == 0:
                val_results = self.evaluate_validation(val_batches)  # æ–°æ–¹æ³•ï¼šä»…åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                val_metrics.append(val_results)
                self.scheduler.step(avg_train_loss)

                print(f"Epoch {epoch}/{num_epochs}:")
                print(f"  ğŸŸ¢ Train Loss = {avg_train_loss:.4f}")
                print(f"  ğŸŸ¡ Val AUC = {val_results['auc']:.4f}, Val Acc = {val_results['accuracy']:.4f}")

        return train_losses, val_metrics

    def evaluate_validation(self, num_batches=10):
        """
        âœ… åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ï¼ˆç”¨äºæ¨¡å‹é€‰æ‹©å’Œæ—©åœï¼‰
        """
        self.model.eval()
        all_scores = []
        all_labels = []
        batch_size = self.config.get('batch_size', 32)

        with torch.no_grad():
            for _ in range(num_batches):
                # ğŸŸ¡ ä»…ä½¿ç”¨éªŒè¯é›†æ•°æ®ï¼ŒæŒ‡å®šæ•°æ®é›†
                images, text_sequences, labels, anomaly_types = \
                    self.data_generator.generate_multimodal_data(
                        batch_size, self.device, split='val', dataset=self.dataset, anomaly_ratio=0.2
                    )

                scores, _ = self.model(images, text_sequences)
                all_scores.extend(scores.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        return self._compute_metrics(all_scores, all_labels)

    def evaluate_testset(self, num_batches=10):
        """
        âœ… æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°ï¼ˆä»…åœ¨è®­ç»ƒå®Œæˆåè°ƒç”¨ï¼‰
        è¿™æ˜¯çœŸæ­£çš„æ³›åŒ–æ€§èƒ½æµ‹è¯•
        """
        print(f"ğŸ”´ âš ï¸  æ‰§è¡Œæœ€ç»ˆæµ‹è¯•é›†è¯„ä¼° - è¿™æ˜¯çœŸæ­£çš„æ³›åŒ–æµ‹è¯•!")
        self.model.eval()
        all_scores = []
        all_labels = []
        batch_size = self.config.get('batch_size', 32)

        with torch.no_grad():
            for batch_idx in range(num_batches):
                # ğŸ”´ é¦–æ¬¡æ¥è§¦æµ‹è¯•é›†æ•°æ®ï¼ˆä»…åœ¨æœ€ç»ˆè¯„ä¼°æ—¶ï¼‰ï¼ŒæŒ‡å®šæ•°æ®é›†
                images, text_sequences, labels, anomaly_types = \
                    self.data_generator.generate_multimodal_data(
                        batch_size, self.device, split='test', dataset=self.dataset, anomaly_ratio=0.2
                    )

                scores, _ = self.model(images, text_sequences)
                all_scores.extend(scores.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                if batch_idx == 0:
                    print(f"ğŸ”´ æµ‹è¯•æ‰¹æ¬¡ {batch_idx+1}: {len(labels)} æ ·æœ¬, {labels.sum().item()} ä¸ªå¼‚å¸¸")

        test_results = self._compute_metrics(all_scores, all_labels)
        print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ: AUC={test_results['auc']:.4f}, Accuracy={test_results['accuracy']:.4f}")
        return test_results

    def _compute_metrics(self, all_scores, all_labels):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„é€šç”¨æ–¹æ³•"""
        all_scores = np.array(all_scores)
        all_labels = np.array(all_labels)
        
        results = {}
        
        try:
            results['auc'] = roc_auc_score(all_labels, all_scores)
        except:
            results['auc'] = 0.5

        try:
            from sklearn.metrics import accuracy_score, precision_recall_curve
            
            threshold = 0.5
            predicted_labels_fixed = (all_scores > threshold).astype(int)
            results['accuracy_fixed'] = accuracy_score(all_labels, predicted_labels_fixed)

            precision, recall, thresholds = precision_recall_curve(all_labels, all_scores)
            temp_f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
            best_threshold_idx = np.argmax(temp_f1_scores)
            best_threshold = thresholds[best_threshold_idx] if len(thresholds) > 0 else 0.5

            predicted_labels_optimal = (all_scores > best_threshold).astype(int)
            results['accuracy_optimal'] = accuracy_score(all_labels, predicted_labels_optimal)
            results['best_threshold'] = best_threshold
            results['accuracy'] = results['accuracy_optimal']

        except Exception as e:
            print(f"Warning: Could not calculate accuracy: {e}")
            results['accuracy'] = 0.5
            results['accuracy_fixed'] = 0.5
            results['accuracy_optimal'] = 0.5
            results['best_threshold'] = 0.5

        return results



class BaselineExperiment:
    """åŸºçº¿å®éªŒç®¡ç†å™¨ - åªè¿è¡Œä¸»å®éªŒçš„baseline"""

    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.results = {}

        # åŸºç¡€é…ç½® - é€‚é…BloodMNISTæ•°æ®é›†
        self.base_config = {
            'hidden_dim': 128,
            'repr_dim': 64,
            'final_repr_dim': 32,
            'embedding_dim': 64,
            'max_seq_len': 64,
            'vocab_size': None,
            'text_dim': 64,

            # =================== åšå®šåœ°ä¿®æ”¹ä»¥ä¸‹ä¸‰ä¸ªå‚æ•° ===================
            'top_k': 5,  # å¿…é¡»ä¿®æ”¹ã€‚ä»10æ”¹ä¸º5ï¼Œæ„å»ºç¨€ç–é«˜è´¨é‡çš„å›¾ã€‚
            'cheb_k': 3,  # å¿…é¡»ä¿®æ”¹ã€‚ä»10æ”¹ä¸º3ï¼Œèšç„¦å±€éƒ¨ç‰¹å¾ï¼Œé˜²æ­¢è¿‡å¹³æ»‘ã€‚
            'learning_rate': 0.0005,  # å¿…é¡»ä¿®æ”¹ã€‚ä»0.001æ”¹ä¸º0.0005ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
            # ==========================================================

            'threshold': 0.6,  # æ­¤å‚æ•°å¯ä¿æŒä¸å˜
            'k_nearest': 3,
            'tau': 0.5,
            'lambda_recon': 0.1,
            'lambda_spectral': 0.01,
            'batch_size': 32,
            'weight_decay': 1e-5,
            'cache_size': 1024,
            'num_classes': 8,  # é»˜è®¤BloodMNISTï¼Œä¼šåœ¨è¿è¡Œæ—¶åŠ¨æ€æ›´æ–°
            'image_size': 28,
            'lstm_layers': 2,
            'lstm_dropout': 0.3,
            'data_root': r"F:\\Desktop\\bloodmnist\\Data"  # æ•°æ®æ ¹ç›®å½•
        }

        # åªä¿ç•™ä¸»å®éªŒçš„baselineé…ç½® + æ–°å¢åŸºçº¿æ–¹æ³•
        self.baseline_config = {
            'original_baseline_bilstm': {
                'name': 'Complete Model (Baseline)',
                'hypergraph_type': 'dynamic',
                'conv_type': 'wavelet_cheb',
                'use_spectral_regularizer': True,
            }
        }
        
        # æ–°å¢ï¼šåŸºçº¿å¯¹æ¯”æ–¹æ³•é…ç½®
        self.comparison_methods = {
            'autoencoder': {
                'name': 'Autoencoder Baseline',
                'description': 'Simple autoencoder for anomaly detection'
            },
            'svm': {
                'name': 'One-Class SVM',
                'description': 'One-Class Support Vector Machine'
            },
            'isolation_forest': {
                'name': 'Isolation Forest',
                'description': 'Isolation Forest ensemble method'
            }
        }
        
        # å½“å‰ä½¿ç”¨çš„æ•°æ®é›†
        self.current_dataset = 'bloodmnist'

    def run_baseline_experiment(self, epochs=30):
        """è¿è¡Œä¸»å®éªŒçš„baseline"""
        experiment_name = 'original_baseline_bilstm'
        print(f"\n{'=' * 20} {self.baseline_config[experiment_name]['name']} {'=' * 20}")

        baseline_config = self.baseline_config[experiment_name]

        try:
            model = AblationAnomalyDetector(self.base_config, baseline_config).to(self.device)
            print(f"Running baseline experiment using BiLSTM Text Encoder with Word Embeddings.")

            trainer = AblationTrainer(model, self.base_config, self.device, self.current_dataset)

            print(f"Starting training for {epochs} epochs...")

            train_losses, val_metrics = trainer.train(epochs, train_batches=15, val_batches=5)

            final_results = trainer.evaluate_testset(num_batches=32)

            self.results[experiment_name] = {
                'config': baseline_config,
                'train_losses': train_losses,
                'val_metrics': val_metrics,
                'final_results': final_results,
                'val_aucs': [m['auc'] for m in val_metrics],
                'final_auc': final_results['auc'],
            }

            print(f"Baseline experiment completed")
            print(f"Final Results - AUC: {final_results['auc']:.4f}, "
                  f"Accuracy: {final_results['accuracy']:.4f}")

            return final_results

        except Exception as e:
            print(f"Baseline experiment failed: {e}")
            import traceback
            traceback.print_exc()

            default_results = {
                'auc': 0.0,
                'accuracy': 0.0,
                'error': str(e)
            }

            self.results[experiment_name] = {
                'config': baseline_config,
                'train_losses': [],
                'val_metrics': [],
                'final_results': default_results,
                'val_aucs': [],
                'final_auc': 0.0,
            }

            return default_results

    def run_baseline_comparison_methods(self, epochs=30, dataset='bloodmnist'):
        """è¿è¡ŒåŸºçº¿å¯¹æ¯”æ–¹æ³•ï¼ˆAutoencoderã€SVMã€Isolation Forestï¼‰"""
        print(f"\nğŸ”¬ å¼€å§‹è¿è¡ŒåŸºçº¿å¯¹æ¯”æ–¹æ³• - {dataset.upper()}")
        print("=" * 70)
        
        # åˆ›å»ºåŸºçº¿æ¨¡å‹è®­ç»ƒå™¨
        baseline_trainer = BaselineModelTrainer(self.base_config, self.device)
        comparison_results = {}
        
        # 1. è®­ç»ƒå’Œè¯„ä¼°è‡ªç¼–ç å™¨
        print("\n1ï¸âƒ£ è‡ªç¼–ç å™¨åŸºçº¿å®éªŒ")
        print("-" * 50)
        try:
            autoencoder_results = baseline_trainer.train_autoencoder(
                dataset=dataset, epochs=epochs, batch_size=self.base_config['batch_size']
            )
            comparison_results['autoencoder'] = {
                'name': self.comparison_methods['autoencoder']['name'],
                'results': autoencoder_results
            }
            print(f"âœ… è‡ªç¼–ç å™¨å®Œæˆ - AUC: {autoencoder_results['auc']:.4f}, Accuracy: {autoencoder_results['accuracy']:.4f}")
        except Exception as e:
            print(f"âŒ è‡ªç¼–ç å™¨å¤±è´¥: {str(e)}")
            comparison_results['autoencoder'] = {
                'name': self.comparison_methods['autoencoder']['name'],
                'results': {'auc': 0.0, 'accuracy': 0.0, 'error': str(e)}
            }
        
        # 2. è®­ç»ƒå’Œè¯„ä¼°One-Class SVM
        print("\n2ï¸âƒ£ One-Class SVMåŸºçº¿å®éªŒ")
        print("-" * 50)
        try:
            svm_results = baseline_trainer.train_svm(
                dataset=dataset, batch_size=self.base_config['batch_size'], nu=0.1
            )
            comparison_results['svm'] = {
                'name': self.comparison_methods['svm']['name'],
                'results': svm_results
            }
            print(f"âœ… SVMå®Œæˆ - AUC: {svm_results['auc']:.4f}, Accuracy: {svm_results['accuracy']:.4f}")
        except Exception as e:
            print(f"âŒ SVMå¤±è´¥: {str(e)}")
            comparison_results['svm'] = {
                'name': self.comparison_methods['svm']['name'],
                'results': {'auc': 0.0, 'accuracy': 0.0, 'error': str(e)}
            }
        
        # 3. è®­ç»ƒå’Œè¯„ä¼°Isolation Forest
        print("\n3ï¸âƒ£ Isolation ForeståŸºçº¿å®éªŒ")
        print("-" * 50)
        try:
            isolation_results = baseline_trainer.train_isolation_forest(
                dataset=dataset, batch_size=self.base_config['batch_size'], contamination=0.1
            )
            comparison_results['isolation_forest'] = {
                'name': self.comparison_methods['isolation_forest']['name'],
                'results': isolation_results
            }
            print(f"âœ… Isolation Forestå®Œæˆ - AUC: {isolation_results['auc']:.4f}, Accuracy: {isolation_results['accuracy']:.4f}")
        except Exception as e:
            print(f"âŒ Isolation Forestå¤±è´¥: {str(e)}")
            comparison_results['isolation_forest'] = {
                'name': self.comparison_methods['isolation_forest']['name'],
                'results': {'auc': 0.0, 'accuracy': 0.0, 'error': str(e)}
            }
        
        return comparison_results

    def run_baseline_only(self, epochs=30, dataset='bloodmnist'):
        """è¿è¡Œä¸»å®éªŒçš„baseline + åŸºçº¿å¯¹æ¯”æ–¹æ³•"""
        print(f"Starting comprehensive baseline experiment on {dataset.upper()}, device: {self.device}")

        # åŠ¨æ€è·å– vocab_size å¹¶æ›´æ–° base_config
        temp_data_loader = MultiDatasetLoader(
            data_root=self.base_config.get('data_root', "F:\\Desktop\\bloodmnist\\Data"),
            batch_size=self.base_config['batch_size'],
            cache_size=self.base_config['cache_size'],
            embedding_dim=self.base_config['embedding_dim'],
            max_seq_len=self.base_config['max_seq_len']
        )
        vocab_size = temp_data_loader.vocab_size  # ç›´æ¥è®¿é—®è¯æ±‡è¡¨å¤§å°å±æ€§
        self.base_config['vocab_size'] = vocab_size
        
        # è·å–æ•°æ®é›†ä¿¡æ¯å¹¶æ›´æ–°é…ç½®
        dataset_info = temp_data_loader.get_dataset_info(dataset)
        self.base_config['num_classes'] = dataset_info['num_classes']
        self.current_dataset = dataset
        
        print(f"Dynamic vocabulary size detected: {vocab_size}")
        print(f"Dataset info: {dataset_info}")
        del temp_data_loader

        # 1. è¿è¡Œä¸»è¦çš„baselineå®éªŒï¼ˆåŸæœ‰çš„hypergraphæ¨¡å‹ï¼‰
        print(f"\nğŸ¯ ä¸»è¦åŸºçº¿å®éªŒ - å¤šæ¨¡æ€è¶…å›¾å¼‚å¸¸æ£€æµ‹")
        print("=" * 70)
        try:
            main_results = self.run_baseline_experiment(epochs)
            self.results['main_model'] = self.results.get('original_baseline_bilstm', {})
        except Exception as e:
            print(f"Main baseline experiment failed: {str(e)}")
            self.results['main_model'] = {'final_results': {'auc': 0.0, 'accuracy': 0.0, 'error': str(e)}}

        # 2. è¿è¡ŒåŸºçº¿å¯¹æ¯”æ–¹æ³•
        comparison_results = self.run_baseline_comparison_methods(epochs=epochs//2, dataset=dataset)
        self.results['comparison_methods'] = comparison_results

        return self.results

def main():
    """Main function - åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œbaselineå®éªŒ"""

    print("ğŸ§ª Multimodal Hypergraph Anomaly Detection System - Multi-Dataset Baseline Experiments")
    print("================================================================================")
    print("Running baseline experiments on BloodMNIST, PathMNIST, and OrganaMNIST datasets.")

    # ä¸‰ä¸ªæ•°æ®é›†åˆ—è¡¨
    datasets = ['bloodmnist', 'pathmnist', 'organamnist']
    all_results = {}

    for dataset in datasets:
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ å¼€å§‹åœ¨ {dataset.upper()} æ•°æ®é›†ä¸Šè¿è¡ŒåŸºçº¿å®éªŒ")
        print(f"{'='*80}")
        
        experiment = BaselineExperiment()
        
        print(f"Starting baseline experiment with {dataset.upper()} dataset...")
        results = experiment.run_baseline_only(epochs=50, dataset=dataset)
        
        # å­˜å‚¨ç»“æœ
        all_results[dataset] = results

        # æ‰“å°å½“å‰æ•°æ®é›†ç»“æœ
        print(f"\n{'='*80}")
        print(f"{dataset.upper()} COMPREHENSIVE BASELINE EXPERIMENT RESULTS")
        print(f"{'='*80}")
        
        # ä¸»æ¨¡å‹ç»“æœ
        if 'main_model' in results and 'final_results' in results['main_model']:
            main_result = results['main_model']['final_results']
            print(f"ğŸ¯ ä¸»è¦æ¨¡å‹ (å¤šæ¨¡æ€è¶…å›¾): AUC = {main_result['auc']:.4f}, Accuracy = {main_result['accuracy']:.4f}")
        else:
            print(f"âŒ ä¸»è¦æ¨¡å‹: å®éªŒå¤±è´¥")
        
        # å¯¹æ¯”æ–¹æ³•ç»“æœ
        if 'comparison_methods' in results:
            print(f"\nğŸ“Š åŸºçº¿å¯¹æ¯”æ–¹æ³•:")
            comparison_methods = results['comparison_methods']
            
            for method_key, method_data in comparison_methods.items():
                method_name = method_data['name']
                method_results = method_data['results']
                if 'error' in method_results:
                    print(f"   âŒ {method_name}: å®éªŒå¤±è´¥ - {method_results.get('error', 'Unknown error')}")
                else:
                    print(f"   âœ… {method_name}: AUC = {method_results['auc']:.4f}, Accuracy = {method_results['accuracy']:.4f}")
        
        print(f"{'='*80}")

    # æ±‡æ€»æ‰€æœ‰ç»“æœ
    print(f"\n{'='*80}")
    print("ï¿½ ALL DATASETS SUMMARY")
    print(f"{'='*80}")
    
    for dataset in datasets:
        if dataset in all_results:
            results = all_results[dataset]
            print(f"\nğŸ“‹ {dataset.upper()} æ•°æ®é›†:")
            
            # ä¸»æ¨¡å‹ç»“æœ
            if 'main_model' in results and 'final_results' in results['main_model']:
                main_result = results['main_model']['final_results']
                print(f"   ğŸ¯ å¤šæ¨¡æ€è¶…å›¾æ¨¡å‹: AUC = {main_result['auc']:.4f}, Accuracy = {main_result['accuracy']:.4f}")
            else:
                print(f"   âŒ å¤šæ¨¡æ€è¶…å›¾æ¨¡å‹: å¤±è´¥")
            
            # å¯¹æ¯”æ–¹æ³•ç»“æœ
            if 'comparison_methods' in results:
                comparison_methods = results['comparison_methods']
                for method_key, method_data in comparison_methods.items():
                    method_name = method_data['name']
                    method_results = method_data['results']
                    if 'error' not in method_results:
                        print(f"   ğŸ“Š {method_name}: AUC = {method_results['auc']:.4f}, Accuracy = {method_results['accuracy']:.4f}")
                    else:
                        print(f"   âŒ {method_name}: å¤±è´¥")
        else:
            print(f"âŒ {dataset.upper():12} - Failed to complete")

    print(f"\nğŸ‰ Multi-dataset comprehensive baseline experiments completed!")
    print("âœ… All three datasets have been evaluated with multiple baseline methods")


if __name__ == "__main__":
    main()